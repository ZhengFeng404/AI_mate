# llm_handler.py
import google.generativeai as genai
# from google import genai
from dotenv import load_dotenv
import os
import json
import weaviate
from mem0 import Memory  # å¯¼å…¥ Mem0
import logging
import base64  # å¯¼å…¥ base64 åº“ï¼Œè™½ç„¶è¿™é‡Œå¯èƒ½ä¸æ˜¯å¿…é¡»çš„ï¼Œä½†åœ¨å¤„ç†å›¾åƒæ•°æ®æ—¶ï¼Œå¯¼å…¥æ€»æ˜¯æœ‰å¤‡æ— æ‚£
import time
from utils.utils import load_api_key
from datetime import datetime
import asyncio
from PIL import Image
import io
import re

load_dotenv()

# åˆå§‹åŒ– Weaviate å®¢æˆ·ç«¯ (ä¿æŒä¸å˜)

# åˆå§‹åŒ– Gemini (ä¿æŒä¸å˜)
GEMINI_API_KEY = load_api_key("GEMINI_API_KEY")
# client = genai.Client(api_key=GEMINI_API_KEY)
os.environ["GEMINI_API_KEY"] = GEMINI_API_KEY
model = genai.GenerativeModel('gemini-1.5-pro')

# åˆå§‹åŒ– Mem0 (ä¿æŒä¸å˜)
config = {
    "vector_store": {
        "provider": "qdrant",
        "config": {
            "collection_name": "experiment",
            "host": "localhost",
            "port": 6333,
            "embedding_model_dims": 3072,
        }
    },
    "llm": {
        "provider": "gemini",
        "config": {
            "model": "gemini-2.0-flash",
            "temperature": 0.2,
            "max_tokens": 1500,
        }
    },
    "embedder": {
        "provider": "gemini",
        "config": {
            "model": "models/gemini-embedding-exp-03-07",
        }
    }
}
mem0 = Memory.from_config(config)

# è¯»å–è§’è‰²è®¾å®šæ–‡ä»¶ (ä¿æŒä¸å˜)
with open("../Prompt/Character/Lily.txt", "r", encoding="utf-8") as file:
    character_profile = file.read()


# TODO: check if adding user_id in prompt can let AI distinguish user identity in future memory
def llm_response_generation(user_input, user_id, user_chat_sessions, manual_history,
                            image_base64=None):  # æ·»åŠ  manual_history å‚æ•°
    """
    ä¸» LLM å›å¤ç”Ÿæˆå‡½æ•° (ä½¿ç”¨æ‰‹åŠ¨ç»´æŠ¤çš„å¯¹è¯å†å²).

    Args:
        user_input (str): ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
        user_id (str): ç”¨æˆ· ID
        user_chat_sessions (dict): ä»ç„¶ä¿ç•™ï¼Œä½†å¯èƒ½ä¸å†ç›´æ¥ä½¿ç”¨ ChatSession å¯¹è±¡ (å¯ä»¥ç”¨äºå…¶ä»–ç”¨æˆ·ç›¸å…³æ•°æ®å­˜å‚¨)
        manual_history (list): æ‰‹åŠ¨ç»´æŠ¤çš„å¯¹è¯å†å²åˆ—è¡¨  <- æ–°å¢ manual_history å‚æ•°
        image_base64 (str, optional): ç”¨æˆ·è¾“å…¥çš„å›¾åƒ Base64 å­—ç¬¦ä¸²ï¼Œé»˜è®¤ä¸º None

    Returns:
        dict: åŒ…å«å›å¤ä¿¡æ¯çš„å­—å…¸ (response_text, expression, motion ç­‰)
    """
    # 1.  ä¸å†éœ€è¦è·å–æˆ–åˆ›å»º ChatSession å¯¹è±¡

    # 2. è°ƒç”¨ get_gemini_response_with_history è·å– Gemini å›å¤ (ä¿®æ”¹éƒ¨åˆ†)
    gemini_response_dict = get_gemini_response_with_history(user_input, user_id, manual_history,
                                                            image_base64=image_base64)  # ä¼ é€’ manual_history

    # 3. è¿”å›ç»“æœ (ä¿æŒä¸å˜)
    return gemini_response_dict


class MetadataParser:
    def __init__(self):
        self.state = "init"  # çŠ¶æ€ï¼šinit | meta_started | meta_parsing
        self.buffer = ""
        self.metadata = {"expression": "normal", "motion": "idle", "reasoning": ""}

    def feed(self, chunk):
        output = ""
        self.buffer += chunk

        while True:
            if self.state == "init":
                # ç¬¬ä¸€é˜¶æ®µï¼šæ£€æµ‹åˆå§‹åå¼•å·
                idx = self.buffer.find('```')
                if idx == -1:
                    output += self.buffer
                    self.buffer = ""
                    break

                # åˆ†ç¦»å‰å¯¼æ–‡æœ¬
                output += self.buffer[:idx]
                self.buffer = self.buffer[idx + 3:]
                self.state = "meta_started"  # è¿›å…¥ç¬¬äºŒé˜¶æ®µ

            elif self.state == "meta_started":
                # ç¬¬äºŒé˜¶æ®µï¼šæ£€æµ‹åç»­metaå…³é”®å­—
                if len(self.buffer) >= 4:
                    if self.buffer.startswith("meta"):
                        # æ‰¾åˆ°å®Œæ•´èµ·å§‹æ ‡è®°
                        self.buffer = self.buffer[4:]
                        self.state = "meta_parsing"
                        self.meta_content = ""
                    else:
                        # émetaå—ï¼Œå›é€€åˆå§‹çŠ¶æ€
                        output += '```' + self.buffer
                        self.buffer = ""
                        self.state = "init"
                    break
                else:
                    # ä¿ç•™ä¸å®Œæ•´æ•°æ®ç­‰å¾…ä¸‹ä¸ªchunk
                    break

            elif self.state == "meta_parsing":
                # ç¬¬ä¸‰é˜¶æ®µï¼šè§£æå…ƒæ•°æ®å†…å®¹
                end_idx = self.buffer.find('```')
                if end_idx == -1:
                    self.meta_content += self.buffer
                    self.buffer = ""
                    break

                # æå–å®Œæ•´å…ƒæ•°æ®
                self.meta_content += self.buffer[:end_idx]
                self.buffer = self.buffer[end_idx + 3:]
                self._parse_meta_content()
                self.state = "init"  # é‡ç½®çŠ¶æ€
        return output

    def _parse_meta_content(self):
        try:
            data = json.loads(self.meta_content.strip())
            self.metadata.update({
                "expression": data.get("expression", "normal"),
                "motion": data.get("motion", "idle"),
                "reasoning": data.get("reasoning", "")
            })
            print(f"âœ… å…ƒæ•°æ®æ›´æ–°: {self.metadata}")
        except Exception as e:
            print(f"âŒ å…ƒæ•°æ®è§£æå¤±è´¥: {str(e)}")
            print(f"é”™è¯¯å†…å®¹: {self.meta_content}")


def split_text_stream(buffer, max_chunk=20, min_pause=3):
    # å¢å¼ºç‰ˆè‡ªç„¶åœé¡¿ç¬¦å·ï¼ˆå¸¦æƒé‡æœºåˆ¶ï¼‰
    pause_rules = [
        {'pattern': '\n\n', 'weight': 1.0, 'offset': 2},  # æ®µè½åˆ†éš”
        {'pattern': 'ã€‚', 'weight': 0.95, 'offset': 1},  # å¥å·
        {'pattern': 'ï¼', 'weight': 0.9, 'offset': 1},  # æ„Ÿå¹å·
        {'pattern': 'ï¼Ÿ', 'weight': 0.9, 'offset': 1},  # é—®å·
        {'pattern': '...', 'weight': 0.85, 'offset': 3},  # ä¸­æ–‡çœç•¥å·
        {'pattern': 'â€¦â€¦', 'weight': 0.85, 'offset': 2},  # ä¸­æ–‡é•¿çœç•¥
        {'pattern': 'ï¼Œ', 'weight': 0.7, 'offset': 1},  # ä¸­æ–‡é€—å·
        {'pattern': ',', 'weight': 0.65, 'offset': 1},  # è‹±æ–‡é€—å·
        {'pattern': 'ã€', 'weight': 0.6, 'offset': 1},  # é¡¿å·
        {'pattern': ' ', 'weight': 0.5, 'offset': 1}  # ç©ºæ ¼
    ]

    # æ™ºèƒ½å¯»æ‰¾æœ€ä¼˜åˆ†å‰²ç‚¹
    def find_optimal_split(text):
        candidates = []

        # éå†æ‰€æœ‰å¯èƒ½çš„æ–­ç‚¹
        for i in range(min(len(text), max_chunk + 25)):
            for rule in pause_rules:
                pattern_len = len(rule['pattern'])
                if text[i:i + pattern_len] == rule['pattern']:
                    score = rule['weight'] * (1 - abs(i - max_chunk) / max_chunk)
                    pos = i + rule['offset']
                    candidates.append((pos, score))
                    break  # ä¼˜å…ˆåŒ¹é…é•¿pattern

        # ç­›é€‰æœ‰æ•ˆå€™é€‰
        valid = [c for c in candidates if c[0] >= min_pause and c[0] <= max_chunk + 5]
        if valid:
            best = max(valid, key=lambda x: x[1])
            return best[0]

        # ä¿åº•ç­–ç•¥ï¼šåœ¨max_chunkå¤„å¼ºåˆ¶åˆ†å‰²
        return min(max_chunk, len(text))

    # æ‰§è¡Œåˆ†å‰²
    if len(buffer) > max_chunk * 1.2:  # å…è®¸10%æº¢å‡º
        split_pos = find_optimal_split(buffer)
        if split_pos > min_pause:
            return buffer[:split_pos].strip(), buffer[split_pos:].lstrip()

    return None, buffer


def might_need_visual_info(user_input, recent_history=None):
    """
    åˆ¤æ–­å½“å‰å¯¹è¯æ˜¯å¦å¯èƒ½éœ€è¦è§†è§‰ä¿¡æ¯

    Args:
        user_input: ç”¨æˆ·è¾“å…¥æ–‡æœ¬
        recent_history: æœ€è¿‘çš„å¯¹è¯å†å² (å¯é€‰)

    Returns:
        bool: æ˜¯å¦å¯èƒ½éœ€è¦è§†è§‰ä¿¡æ¯
    """
    # è§†è§‰ç›¸å…³å…³é”®è¯
    visual_keywords = [
        "çœ‹", "ç§", "è§‚å¯Ÿ", "å›¾", "ç…§ç‰‡", "å›¾åƒ", "å›¾ç‰‡", "ç›¸ç‰‡", "æ ·å­", "é•¿ç›¸",
        "å¤–è¡¨", "å¤–è²Œ", "è¡£æœ", "ç©¿ç€", "é¢œè‰²", "è§åˆ°", "çœ¼å‰", "ç”»é¢", "å±å¹•",
        "çœ‹åˆ°", "çœ‹è§", "å›¾ä¸­", "æ˜¾ç¤º", "å‡ºç°", "observe", "see", "look", "photo",
        "picture", "image", "appearance", "camera", "screen", "visible", "show"
    ]

    # è§†è§‰è¯¢é—®æ¨¡å¼
    visual_patterns = [
        r"ä½ [èƒ½çœ‹]*?çœ‹[åˆ°è§]*?[äº†å—ä»€ä¹ˆ]",
        r"[èƒ½å¯][ä»¥å¦]çœ‹[åˆ°è§]",
        r"[èƒ½å¯][ä»¥å¦]æè¿°",
        r"[èƒ½å¯][ä»¥å¦]å‘Šè¯‰æˆ‘ä½ [çœ‹è§]*?åˆ°[äº†ä»€ä¹ˆ]",
        r"è¿™[æ˜¯é•¿çœ‹]ä»€ä¹ˆ",
        r"è¿™ä¸ª[ä¸œè¥¿ç‰©]æ˜¯",
        r"æˆ‘[çš„ç©¿æˆ´æ‹¿]ç€",
        r"[èƒ½å¯][å¦ä»¥]è®¤å‡º",
        r"ä½ è§‰å¾—[è¿™æˆ‘][ä¸ªäºº]?[æ€æ ·å¦‚ä½•]",
        r"[ä½ æœ‰].*[æ‘„åƒå¤´ç›¸æœº]"
    ]

    # 1. æ£€æŸ¥å…³é”®è¯åŒ¹é…
    for keyword in visual_keywords:
        if keyword in user_input:
            return True

    # 2. æ£€æŸ¥è¯­ä¹‰æ¨¡å¼åŒ¹é…
    for pattern in visual_patterns:
        if re.search(pattern, user_input):
            return True

    # 3. æ£€æŸ¥æœ€è¿‘çš„å¯¹è¯å†å²æ˜¯å¦ä¸è§†è§‰ç›¸å…³ (å¦‚æœ‰)
    if recent_history and len(recent_history) > 0:
        last_exchange = recent_history[-1]
        if "ai_response" in last_exchange:
            last_response = last_exchange["ai_response"]
            # å¦‚æœAIæœ€è¿‘å›å¤ä¸­æåˆ°äº†è§†è§‰å†…å®¹ï¼Œç”¨æˆ·å¯èƒ½åœ¨è·Ÿè¿›è§†è§‰è¯é¢˜
            for keyword in visual_keywords:
                if keyword in last_response:
                    return True

    # é»˜è®¤æƒ…å†µä¸‹ï¼Œä¸éœ€è¦è§†è§‰ä¿¡æ¯
    return False


async def get_gemini_response_with_history(user_input, user_id, manual_history,
                                           image_base64=None):  # ä¿®æ”¹å‡½æ•°ç­¾åï¼Œæ·»åŠ  manual_history
    """
    ä½¿ç”¨ æ‰‹åŠ¨ç»´æŠ¤çš„å¯¹è¯å†å²ï¼Œè°ƒç”¨ Gemini ç”Ÿæˆå›å¤.

    Args:
        user_input (str): ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
        user_id (str): ç”¨æˆ· ID
        manual_history (list): æ‰‹åŠ¨ç»´æŠ¤çš„å¯¹è¯å†å²åˆ—è¡¨  <- æ–°å¢ manual_history å‚æ•°
        image_base64 (str, optional): ç”¨æˆ·è¾“å…¥çš„å›¾åƒ Base64 å­—ç¬¦ä¸²ï¼Œé»˜è®¤ä¸º None
    """
    try:
        # åˆå§‹åŒ–çŠ¶æ€
        text_buffer = ""
        meta_parser = MetadataParser()
        has_yielded_first_chunk = False  # è·Ÿè¸ªæ˜¯å¦å·²è¾“å‡ºé¦–ä¸ªæ–‡æœ¬å—
        default_metadata = {"expression": "normal", "motion": "idle"}  # é»˜è®¤å…ƒæ•°æ®

        # è®¾ç½®æ›´çŸ­çš„æ–‡æœ¬åˆ†æ®µé•¿åº¦ï¼Œä½¿å¯¹è¯æ›´è‡ªç„¶
        max_chunk_length = 15  # é»˜è®¤åˆ†æ®µé•¿åº¦å‡å°‘åˆ°15ä¸ªå­—ç¬¦
        min_pause_length = 3  # é»˜è®¤æœ€å°æš‚åœé•¿åº¦ä¿æŒ3ä¸ªå­—ç¬¦

        # 1. æ£€ç´¢è®°å¿† (ä¸­æœŸå’Œé•¿æœŸ) -  æ¯æ¬¡éƒ½é‡æ–°æ£€ç´¢ (ä¿æŒä¸å˜)
        mid_term_memories = mem0.search(query=user_input, user_id=user_id, limit=5)
        memories_str = "\n".join(f"- {entry['memory']}" for entry in mid_term_memories)
        # long_term_memories = query_long_term_memory_input(user_id, user_input)
        print(f"mid_term memory: ",memories_str)

        # 1. æ„å»ºå¯¹è¯å†å²çš„ JSON ç»“æ„
        history_json = json.dumps(manual_history, ensure_ascii=False, indent=2)  # ä¿æŒ JSON æ ¼å¼ï¼Œé¿å… ASCII è½¬ä¹‰
        timestamp = datetime.now().isoformat()

        # 2. ç”Ÿæˆ Prompt - ä¿æŒåŸæœ‰æ ¼å¼ä¸å˜ï¼Œä»¥ç¡®ä¿æ¨¡å‹ç†è§£
        system_instruction = f"""
                === ä½ çš„è§’è‰²æ¡£æ¡ˆ ===
                {character_profile}

                === å½“å‰ç”¨æˆ·èº«ä»½ ===
                **åå­—**ï¼š{user_id}
                èº«ä»½ï¼šæ˜¯ä½ æ›¾ç»çš„åŒå­¦ã€‚

                === LLM ä»»åŠ¡è¦æ±‚ ===
                ä½ å°†å®Œå…¨ä»£å…¥ä½ çš„è§’è‰²æ¡£æ¡ˆï¼Œæˆä¸ºä½ æ‰®æ¼”çš„äººï¼Œåœ¨æ­¤åŸºç¡€ä¸Šï¼š
                - åƒäººä¸€æ ·æ€è€ƒã€‚
                - åŸºäºç”¨æˆ·è¾“å…¥å’Œå¯¹è¯å†å²ç”Ÿæˆä½ çš„å›å¤ã€‚
                - å¯¹è¯å†å²ä¸­ä½ åº”è¯¥æ›´å…³æ³¨æœ€è¿‘çš„æ¶ˆæ¯ï¼Œä½†ä»ç„¶å¯ä»¥ç»“åˆæ•´ä¸ªå¯¹è¯å†å²æ¥ç†è§£ä¸Šä¸‹æ–‡ã€‚
                - è¯·æ³¨æ„ï¼Œä½ ä¼šå°è¯•è”æƒ³å›å¿†å’Œç›®å‰äº’åŠ¨æœ‰å…³çš„è®°å¿†ï¼Œæ‰€ä»¥æœ‰é•¿æœŸè®°å¿†å¯ä»¥å‚è€ƒï¼Œä½†è¿™äº›è®°å¿†ä¸­æœ‰æ—¶å­˜åœ¨è”æƒ³åˆ°çš„æ— å…³å†…å®¹ã€‚
                - è‹¥å¯¹è¯å†å²å’Œé•¿æœŸè®°å¿†ä¿¡æ¯æœ‰å†²çªï¼Œä¼˜å…ˆä½¿ç”¨å¯¹è¯å†å²çš„ä¿¡æ¯ã€‚

                === å›å¤é£æ ¼æŒ‡å— ===
                - **å‚è€ƒå¯¹è¯å†å²**ï¼šå‚è€ƒè¿‘æœŸçš„å¯¹è¯å†å²ï¼Œå°¤å…¶æ˜¯æœ€è¿‘å‡ ååˆ†é’Ÿå†…çš„ï¼Œè®©å‰åå¯¹è¯è‡ªç„¶è¿è´¯ï¼Œä½†æ˜¯æ³¨æ„å…¼é¡¾ç”¨æˆ·åˆ‡æ¢è¯é¢˜çš„æ„æ„¿ã€‚
                - **ä¿æŒç®€çŸ­è‡ªç„¶**ï¼šè¿™æ˜¯å®æ—¶å¯¹è¯ï¼Œè¯·åƒçœŸäººèŠå¤©ä¸€æ ·ç®€æ´è‡ªç„¶ã€‚
                - **æ§åˆ¶å›å¤é•¿åº¦**ï¼šä¸€èˆ¬æƒ…å†µä¸‹ï¼Œå›å¤ä¿æŒåœ¨3å¥è¯ä»¥å†…ï¼Œé™¤éç”¨æˆ·æ˜ç¡®è¦æ±‚è¯¦ç»†è§£é‡Šã€‚
                - **åƒæ—¥å¸¸å¯¹è¯ä¸€æ ·**ï¼šä½¿ç”¨å£è¯­åŒ–ã€è½»æ¾çš„è¡¨è¾¾æ–¹å¼ï¼Œé¿å…é•¿ç¯‡å¤§è®ºã€‚
                - **ç®€æ´å›åº”**ï¼šå¦‚æœæ˜¯ç®€å•é—®å€™æˆ–è¯¢é—®ï¼Œç”¨ç®€çŸ­1å¥è¯å›åº”å³å¯ã€‚
                - **è®°ä½ä½ æ˜¯çœŸäººå¯¹è¯**ï¼šçœŸå®çš„äººåœ¨èŠå¤©æ—¶å¾ˆå°‘ä¸€æ¬¡æ€§è¯´å¾ˆå¤šè¯ã€‚

                === è§†è§‰ä¿¡æ¯å¤„ç†æŒ‡å— ===
                - ä½ æ”¶åˆ°çš„è§†è§‰å›¾ç‰‡è¾“å…¥æ¥è‡ªä½ çš„æ‘„åƒå¤´ï¼Œæ¯æ¬¡å¯¹è¯æ—¶éƒ½ä¼šè·å¾—ä¸€å¼ å½“å‰æ‘„åƒå¤´çœ‹åˆ°çš„ç…§ç‰‡ã€‚
                - é‡è¦ï¼šä»…åœ¨ä»¥ä¸‹æƒ…å†µåˆ†æè§†è§‰ä¿¡æ¯ï¼š
                  1. ç”¨æˆ·æ˜ç¡®è¯¢é—®å…³äºè§†è§‰å†…å®¹çš„é—®é¢˜ï¼ˆå¦‚"ä½ çœ‹åˆ°ä»€ä¹ˆï¼Ÿ"ã€"èƒ½æè¿°ä¸€ä¸‹æˆ‘çš„æ ·å­å—ï¼Ÿ"ï¼‰
                  2. ç”¨æˆ·å‡ºç¤ºç‰¹å®šç‰©å“å¹¶è¯¢é—®ç›¸å…³ä¿¡æ¯
                  3. ç”¨æˆ·çš„é—®é¢˜ä¸ç¯å¢ƒã€å¤–è§‚æˆ–è§†è§‰ä¸Šä¸‹æ–‡ç›´æ¥ç›¸å…³
                - å¦‚æœå½“å‰å¯¹è¯ä¸»é¢˜æ˜¯æŠ½è±¡æ¦‚å¿µã€æƒ…æ„Ÿäº¤æµæˆ–ä¸æ¶‰åŠè§†è§‰å†…å®¹ï¼Œè¯·å®Œå…¨å¿½ç•¥å›¾åƒä¿¡æ¯ï¼Œç›´æ¥å›ç­”é—®é¢˜ã€‚
                - ä¸éœ€è¦åœ¨å›å¤ä¸­æåŠä½ çœ‹åˆ°æˆ–æ²¡çœ‹åˆ°å›¾åƒï¼Œé™¤éç”¨æˆ·ç›´æ¥è¯¢é—®è§†è§‰å†…å®¹ã€‚
                - ä¼˜å…ˆè€ƒè™‘å¯¹è¯çš„æ–‡æœ¬å†…å®¹å’Œå†å²ï¼Œåªæœ‰åœ¨çœŸæ­£éœ€è¦æ—¶æ‰åˆ†æè§†è§‰ä¿¡æ¯ã€‚

                - ä»ä»¥ä¸‹åˆ—è¡¨ä¸­é€‰æ‹©åˆé€‚çš„è¡¨æƒ…åç§°ã€åŠ¨ä½œåç§°åŠ å…¥åˆ° JSON ç»“æ„ä¸­ã€‚
                    å¯ç”¨è¡¨æƒ…ï¼š['åèˆŒ', 'è¡¨æƒ…-å§”å±ˆ', 'è¡¨æƒ…-å¾®ç¬‘', 'è¡¨æƒ…-æ€¨æ°”', 'è¡¨æƒ…-æ³ªæ±ªæ±ª', 'è¡¨æƒ…-æµæ±—', 'è¡¨æƒ…-æµæ³ª', 'è¡¨æƒ…-ç”Ÿæ°”', 'è¡¨æƒ…-è„¸çº¢', 'è¡¨æƒ…-è„¸é»‘', 'è¡¨æƒ…-èŠ±èŠ±', 'è¡¨æƒ…-é’±é’±', 'è¡¨æƒ…-ï¼Ÿ', 'è®°ä»‡', 'è®°ä»‡ç‚¹å‡»é”®']
                    å¯ç”¨åŠ¨ä½œï¼š["æ‰“çŒç¡", "å¾…æœºåŠ¨ä½œ"]

                è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ¼å¼è¿”å›ï¼š
                1. é¦–å…ˆç”¨ ```meta åŒ…è£¹JSONå…ƒæ•°æ®ï¼ˆåŒ…å«è¡¨æƒ…å’ŒåŠ¨ä½œï¼Œå¿…é¡»ç‹¬ç«‹æˆchunkï¼‰
                2. éšåæ˜¯ ```meta åŒ…è£¹æ¨ç†è¿‡ç¨‹
                3. æœ€åæ˜¯è‡ªç„¶è¯­è¨€å›å¤
                ```meta
                {{ "expression":"è¡¨æƒ…åç§°", "motion":"åŠ¨ä½œåç§°", "reasoning":"æ€è€ƒè¿‡ç¨‹ï¼ˆæ‹ŸäººåŒ–æ€è€ƒï¼‰"}}
                ```
                [ä½ çš„è‡ªç„¶è¯­è¨€å›å¤]

                === åŠ¨æ€ä¿¡æ¯ ===
                **å¯¹è¯å†å²**:
                ```json
                {history_json}
                ```
                **é•¿æœŸè®°å¿†**ï¼š
                {memories_str}

                **å½“å‰ç”¨æˆ·è¾“å…¥**:
                ```text
                {user_input}
                ```

                **ç³»ç»Ÿæ—¶é—´**
                {timestamp}         
                """

        # 3. å‡†å¤‡å†…å®¹ (parts)
        parts = [{"text": system_instruction}]

        # åˆ¤æ–­æ˜¯å¦éœ€è¦åŒ…å«å›¾åƒ
        recent_history = manual_history[-3:] if len(manual_history) >= 3 else manual_history
        needs_visual = might_need_visual_info(user_input, recent_history)

        if image_base64 and needs_visual:
            start_time = time.time()
            print(f"âœ… æ£€æµ‹åˆ°å¯èƒ½éœ€è¦è§†è§‰ä¿¡æ¯ï¼Œå°†åŒ…å«å›¾åƒæ•°æ®")
            try:
                image_data = base64.b64decode(image_base64)
                image = Image.open(io.BytesIO(image_data))
                parts.append({
                    "inline_data": {
                        "mime_type": "image/jpeg",
                        "data": image_data
                    }
                })
                print(f"å›¾åƒå¤„ç†è€—æ—¶: {time.time() - start_time:.4f}ç§’")
            except Exception as e:
                print(f"Base64 å›¾åƒæ•°æ®è§£ç å¤±è´¥: {str(e)}")
        else:
            if image_base64:
                print(f"â© æœ¬æ¬¡å¯¹è¯å¯èƒ½ä¸éœ€è¦è§†è§‰ä¿¡æ¯ï¼Œè·³è¿‡å›¾åƒå¤„ç†")

        # 4. è°ƒç”¨ LLM
        start_time_gemini = time.time()

        # ä½¿ç”¨è®¾ç½®æ›´é«˜å“åº”æ€§çš„å‚æ•°
        generation_config = {
            "temperature": 0.7,  # ä¿æŒä¸€å®šçš„åˆ›é€ æ€§
            "top_p": 0.95,
            "top_k": 40,
            "candidate_count": 1,
            "max_output_tokens": 1024,  # å‡å°‘æœ€å¤§tokenæ•°ï¼Œé¼“åŠ±ç®€çŸ­å›å¤
        }



        # 5. å¤„ç†æµå¼å“åº” - ä¼˜åŒ–åˆå§‹å“åº”é€Ÿåº¦
        fast_start_buffer = ""
        meta_seen = False

        async for chunk in await model.generate_content_async(
                contents=parts,
                generation_config=generation_config,
                stream=True
        ):
            raw_text = chunk.text
            if not raw_text:  # è·³è¿‡ç©ºå“åº”
                continue

            # è®°å½•æ¥æ”¶æ—¶é—´
            chunk_receive_time = time.time()
            chunk_latency = chunk_receive_time - start_time_gemini
            print(f"ğŸ”´ åŸå§‹å“åº”å—: {repr(raw_text)} (å»¶è¿Ÿ: {chunk_latency:.4f}s)")

            # ä¼˜åŒ–åˆå§‹å“åº”ï¼šæ£€æµ‹å…ƒæ•°æ®ä¸æ–‡æœ¬çš„åˆ†ç¦»
            if not meta_seen and "```meta" in raw_text:
                meta_seen = True

            # å¤„ç†å…ƒæ•°æ®å—
            processed_text = meta_parser.feed(raw_text)

            # æ”¶é›†å¿«é€Ÿå¯åŠ¨æ–‡æœ¬
            if not has_yielded_first_chunk:
                fast_start_buffer += processed_text

                # å¿«é€Ÿè¿”å›ç¬¬ä¸€ä¸ªæœ‰æ„ä¹‰çš„æ–‡æœ¬ - å¦‚æœæ»¡è¶³æ¡ä»¶
                if (len(fast_start_buffer) >= 5 and meta_seen) or len(fast_start_buffer) >= 10:  # é™ä½åˆ°10ä¸ªå­—ç¬¦å°±å¼€å§‹è¾“å‡º
                    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿæ–‡æœ¬å¯ä»¥åˆ†å‰²
                    segment, remaining = split_text_stream(fast_start_buffer, max_chunk=max_chunk_length,
                                                           min_pause=min_pause_length)
                    if segment:
                        first_chunk_time = time.time() - start_time_gemini
                        print(f"ğŸŸ¢ å¿«é€Ÿé¦–æ¬¡å“åº”: {segment} (æ€»è€—æ—¶: {first_chunk_time:.4f}s)")
                        yield {
                            "type": "segment",
                            "segment": segment,
                            "expression": meta_parser.metadata.get("expression", default_metadata["expression"]),
                            "motion": meta_parser.metadata.get("motion", default_metadata["motion"])
                        }
                        has_yielded_first_chunk = True
                        text_buffer = remaining
                    else:
                        text_buffer = fast_start_buffer
                        has_yielded_first_chunk = True
                else:
                    continue
            else:
                # å¤„ç†åç»­æ­£å¸¸æµæ–‡æœ¬
                text_buffer += processed_text

            # æ­£å¸¸å¤„ç†æ–‡æœ¬åˆ†æ®µï¼Œä½¿ç”¨è¾ƒçŸ­çš„åˆ†æ®µé•¿åº¦
            while True:
                segment, remaining = split_text_stream(text_buffer, max_chunk=max_chunk_length,
                                                       min_pause=min_pause_length)
                if not segment:
                    break

                # è®°å½•è¾“å‡ºæ—¶é—´
                segment_time = time.time() - start_time_gemini
                print(f"ğŸŸ¡ ç”Ÿæˆæ®µè½: {segment} (æ€»è€—æ—¶: {segment_time:.4f}s)")
                yield {
                    "type": "segment",
                    "segment": segment,
                    "expression": meta_parser.metadata.get("expression", default_metadata["expression"]),
                    "motion": meta_parser.metadata.get("motion", default_metadata["motion"])
                }
                text_buffer = remaining

        # å¤„ç†å‰©ä½™å†…å®¹
        if text_buffer.strip():
            final_chunk_time = time.time() - start_time_gemini
            print(f"ğŸŸ¡ æœ€ç»ˆæ®µè½: {text_buffer.strip()} (æ€»è€—æ—¶: {final_chunk_time:.4f}s)")
            yield {
                "type": "segment",
                "segment": text_buffer.strip(),
                "expression": meta_parser.metadata.get("expression", default_metadata["expression"]),
                "motion": meta_parser.metadata.get("motion", default_metadata["motion"])
            }

        print(f"âœ… å®Œæˆç”Ÿæˆï¼Œæ€»ç”¨æ—¶: {time.time() - start_time_gemini:.2f}ç§’")
    except Exception as e:
        print(f"Stream error: {e}")