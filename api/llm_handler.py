# llm_handler.py
import google.generativeai as genai
#from google import genai
from dotenv import load_dotenv
import os
import json
import weaviate
from mem0 import Memory  # å¯¼å…¥ Mem0
import logging
import base64 # å¯¼å…¥ base64 åº“ï¼Œè™½ç„¶è¿™é‡Œå¯èƒ½ä¸æ˜¯å¿…é¡»çš„ï¼Œä½†åœ¨å¤„ç†å›¾åƒæ•°æ®æ—¶ï¼Œå¯¼å…¥æ€»æ˜¯æœ‰å¤‡æ— æ‚£
import time
from utils.utils import load_api_key
from datetime import datetime
import asyncio
from PIL import Image
import io

load_dotenv()

# åˆå§‹åŒ– Weaviate å®¢æˆ·ç«¯ (ä¿æŒä¸å˜)
weaviate_client = weaviate.connect_to_local()

# åˆå§‹åŒ– Gemini (ä¿æŒä¸å˜)
GEMINI_API_KEY = load_api_key("GEMINI_API_KEY")
#client = genai.Client(api_key=GEMINI_API_KEY)
os.environ["GEMINI_API_KEY"] = GEMINI_API_KEY

# åˆå§‹åŒ– Mem0 (ä¿æŒä¸å˜)
config = {
    "vector_store": {
        "provider": "qdrant",
        "config": {
            "collection_name": "test1",
            "host": "localhost",
            "port": 6333,
            "embedding_model_dims": 1024,
        }
    },
    "llm": {
        "provider": "gemini",
        "config": {
            "model": "gemini-2.0-flash",
            "temperature": 0.2,
            "max_tokens": 1500,
        }
    },
    "embedder": {
        "provider": "ollama",
        "config": {
            "model": "mxbai-embed-large:latest",
        }
    }
}
#mem0 = Memory.from_config(config)

# è¯»å–è§’è‰²è®¾å®šæ–‡ä»¶ (ä¿æŒä¸å˜)
with open("../Prompt/Character/Lily.txt", "r", encoding="utf-8") as file:
    character_profile = file.read()

# æŸ¥è¯¢é•¿æœŸè®°å¿† (ä¿æŒä¸å˜)
def query_long_term_memory_input(user_input):
    related_memory = []
    for collection_name in ["Events", "Relationships", "Knowledge", "Goals", "Preferences", "Profile"]:
        collection = weaviate_client.collections.get(collection_name)
        existing_mem = collection.query.hybrid(
            query=f"User: {user_input}",
            limit=2
        )
        related_memory.append(existing_mem)
    return related_memory


# TODO: check if adding user_id in prompt can let AI distinguish user identity in future memory
def llm_response_generation(user_input, user_id, user_chat_sessions, manual_history, image_base64=None): # æ·»åŠ  manual_history å‚æ•°
    """
    ä¸» LLM å›å¤ç”Ÿæˆå‡½æ•° (ä½¿ç”¨æ‰‹åŠ¨ç»´æŠ¤çš„å¯¹è¯å†å²).

    Args:
        user_input (str): ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
        user_id (str): ç”¨æˆ· ID
        user_chat_sessions (dict): ä»ç„¶ä¿ç•™ï¼Œä½†å¯èƒ½ä¸å†ç›´æ¥ä½¿ç”¨ ChatSession å¯¹è±¡ (å¯ä»¥ç”¨äºå…¶ä»–ç”¨æˆ·ç›¸å…³æ•°æ®å­˜å‚¨)
        manual_history (list): æ‰‹åŠ¨ç»´æŠ¤çš„å¯¹è¯å†å²åˆ—è¡¨  <- æ–°å¢ manual_history å‚æ•°
        image_base64 (str, optional): ç”¨æˆ·è¾“å…¥çš„å›¾åƒ Base64 å­—ç¬¦ä¸²ï¼Œé»˜è®¤ä¸º None

    Returns:
        dict: åŒ…å«å›å¤ä¿¡æ¯çš„å­—å…¸ (response_text, expression, motion ç­‰)
    """
    # 1.  ä¸å†éœ€è¦è·å–æˆ–åˆ›å»º ChatSession å¯¹è±¡

    # 2. è°ƒç”¨ get_gemini_response_with_history è·å– Gemini å›å¤ (ä¿®æ”¹éƒ¨åˆ†)
    gemini_response_dict = get_gemini_response_with_history(user_input, user_id, manual_history, image_base64=image_base64) # ä¼ é€’ manual_history

    # 3. è¿”å›ç»“æœ (ä¿æŒä¸å˜)
    return gemini_response_dict


class MetadataParser:
    def __init__(self):
        self.state = "init"  # çŠ¶æ€ï¼šinit | meta_started | meta_parsing
        self.buffer = ""
        self.metadata = {"expression": "normal", "motion": "idle"}

    def feed(self, chunk):
        output = ""
        self.buffer += chunk

        while True:
            if self.state == "init":
                # ç¬¬ä¸€é˜¶æ®µï¼šæ£€æµ‹åˆå§‹åå¼•å·
                idx = self.buffer.find('```')
                if idx == -1:
                    output += self.buffer
                    self.buffer = ""
                    break

                # åˆ†ç¦»å‰å¯¼æ–‡æœ¬
                output += self.buffer[:idx]
                self.buffer = self.buffer[idx + 3:]
                self.state = "meta_started"  # è¿›å…¥ç¬¬äºŒé˜¶æ®µ

            elif self.state == "meta_started":
                # ç¬¬äºŒé˜¶æ®µï¼šæ£€æµ‹åç»­metaå…³é”®å­—
                if len(self.buffer) >= 4:
                    if self.buffer.startswith("meta"):
                        # æ‰¾åˆ°å®Œæ•´èµ·å§‹æ ‡è®°
                        self.buffer = self.buffer[4:]
                        self.state = "meta_parsing"
                        self.meta_content = ""
                    else:
                        # émetaå—ï¼Œå›é€€åˆå§‹çŠ¶æ€
                        output += '```' + self.buffer
                        self.buffer = ""
                        self.state = "init"
                    break
                else:
                    # ä¿ç•™ä¸å®Œæ•´æ•°æ®ç­‰å¾…ä¸‹ä¸ªchunk
                    break

            elif self.state == "meta_parsing":
                # ç¬¬ä¸‰é˜¶æ®µï¼šè§£æå…ƒæ•°æ®å†…å®¹
                end_idx = self.buffer.find('```')
                if end_idx == -1:
                    self.meta_content += self.buffer
                    self.buffer = ""
                    break

                # æå–å®Œæ•´å…ƒæ•°æ®
                self.meta_content += self.buffer[:end_idx]
                self.buffer = self.buffer[end_idx + 3:]
                self._parse_meta_content()
                self.state = "init"  # é‡ç½®çŠ¶æ€
        return output

    def _parse_meta_content(self):
        try:
            data = json.loads(self.meta_content.strip())
            self.metadata.update({
                "expression": data.get("expression", "normal"),
                "motion": data.get("motion", "idle")
            })
            print(f"âœ… å…ƒæ•°æ®æ›´æ–°: {self.metadata}")
        except Exception as e:
            print(f"âŒ å…ƒæ•°æ®è§£æå¤±è´¥: {str(e)}")
            print(f"é”™è¯¯å†…å®¹: {self.meta_content}")


def split_text_stream(buffer, max_chunk=20, min_pause=3):
    # å¢å¼ºç‰ˆè‡ªç„¶åœé¡¿ç¬¦å·ï¼ˆå¸¦æƒé‡æœºåˆ¶ï¼‰
    pause_rules = [
        {'pattern': '\n\n', 'weight': 1.0, 'offset': 2},  # æ®µè½åˆ†éš”
        {'pattern': 'ã€‚', 'weight': 0.95, 'offset': 1},  # å¥å·
        {'pattern': 'ï¼', 'weight': 0.9, 'offset': 1},  # æ„Ÿå¹å·
        {'pattern': 'ï¼Ÿ', 'weight': 0.9, 'offset': 1},  # é—®å·
        {'pattern': '...', 'weight': 0.85, 'offset': 3},  # ä¸­æ–‡çœç•¥å·
        {'pattern': 'â€¦â€¦', 'weight': 0.85, 'offset': 2},  # ä¸­æ–‡é•¿çœç•¥
        {'pattern': 'ï¼Œ', 'weight': 0.7, 'offset': 1},  # ä¸­æ–‡é€—å·
        {'pattern': ',', 'weight': 0.65, 'offset': 1},  # è‹±æ–‡é€—å·
        {'pattern': 'ã€', 'weight': 0.6, 'offset': 1},  # é¡¿å·
        {'pattern': ' ', 'weight': 0.5, 'offset': 1}  # ç©ºæ ¼
    ]

    # æ™ºèƒ½å¯»æ‰¾æœ€ä¼˜åˆ†å‰²ç‚¹
    def find_optimal_split(text):
        candidates = []

        # éå†æ‰€æœ‰å¯èƒ½çš„æ–­ç‚¹
        for i in range(min(len(text), max_chunk + 25)):
            for rule in pause_rules:
                pattern_len = len(rule['pattern'])
                if text[i:i + pattern_len] == rule['pattern']:
                    score = rule['weight'] * (1 - abs(i - max_chunk) / max_chunk)
                    pos = i + rule['offset']
                    candidates.append((pos, score))
                    break  # ä¼˜å…ˆåŒ¹é…é•¿pattern

        # ç­›é€‰æœ‰æ•ˆå€™é€‰
        valid = [c for c in candidates if c[0] >= min_pause and c[0] <= max_chunk + 5]
        if valid:
            best = max(valid, key=lambda x: x[1])
            return best[0]

        # ä¿åº•ç­–ç•¥ï¼šåœ¨max_chunkå¤„å¼ºåˆ¶åˆ†å‰²
        return min(max_chunk, len(text))

    # æ‰§è¡Œåˆ†å‰²
    if len(buffer) > max_chunk * 1.2:  # å…è®¸10%æº¢å‡º
        split_pos = find_optimal_split(buffer)
        if split_pos > min_pause:
            return buffer[:split_pos].strip(), buffer[split_pos:].lstrip()

    return None, buffer


async def get_gemini_response_with_history(user_input, user_id, manual_history, image_base64=None): # ä¿®æ”¹å‡½æ•°ç­¾åï¼Œæ·»åŠ  manual_history
    """
    ä½¿ç”¨ æ‰‹åŠ¨ç»´æŠ¤çš„å¯¹è¯å†å²ï¼Œè°ƒç”¨ Gemini ç”Ÿæˆå›å¤.

    Args:
        user_input (str): ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
        user_id (str): ç”¨æˆ· ID
        manual_history (list): æ‰‹åŠ¨ç»´æŠ¤çš„å¯¹è¯å†å²åˆ—è¡¨  <- æ–°å¢ manual_history å‚æ•°
        image_base64 (str, optional): ç”¨æˆ·è¾“å…¥çš„å›¾åƒ Base64 å­—ç¬¦ä¸²ï¼Œé»˜è®¤ä¸º None
    """
    try:
        # åˆå§‹åŒ–çŠ¶æ€
        text_buffer = ""
        meta_parser = MetadataParser()
        # 1. æ£€ç´¢è®°å¿† (ä¸­æœŸå’Œé•¿æœŸ) -  æ¯æ¬¡éƒ½é‡æ–°æ£€ç´¢ (ä¿æŒä¸å˜)
        # mid_term_memories = mem0.search(query=user_input, user_id="default_user", limit=3)
        # memories_str = "\n".join(f"- {entry['memory']}" for entry in mid_term_memories)
        long_term_memories = query_long_term_memory_input(user_input)

        # 1. æ„å»ºå¯¹è¯å†å²çš„ JSON ç»“æ„
        history_json = json.dumps(manual_history, ensure_ascii=False, indent=2)  # ä¿æŒ JSON æ ¼å¼ï¼Œé¿å… ASCII è½¬ä¹‰
        timestamp = datetime.now().isoformat()
        # 2. ç”Ÿæˆ Prompt
        system_instruction = f"""
                === ä½ çš„è§’è‰²æ¡£æ¡ˆ ===
                {character_profile}

                === å½“å‰ç”¨æˆ·èº«ä»½ ===
                **åå­—**ï¼š{user_id}
                èº«ä»½ï¼šæ˜¯ä½ æ›¾ç»çš„åŒå­¦ã€‚
                
                === LLM ä»»åŠ¡è¦æ±‚ ===
                ä½ å°†å®Œå…¨ä»£å…¥ä½ çš„è§’è‰²æ¡£æ¡ˆï¼Œæˆä¸ºä½ æ‰®æ¼”çš„äººï¼Œåœ¨æ­¤åŸºç¡€ä¸Šï¼š
                - åƒäººä¸€æ ·æ€è€ƒã€‚
                - åŸºäºç”¨æˆ·è¾“å…¥å’Œ *å¯¹è¯å†å²* ç”Ÿæˆä½ çš„å›å¤ã€‚
                - **å¯¹è¯å†å²**ä¸­ä½ åº”è¯¥æ›´å…³æ³¨æœ€è¿‘çš„æ¶ˆæ¯ï¼Œä½†ä»ç„¶å¯ä»¥ç»“åˆæ•´ä¸ªå¯¹è¯å†å²æ¥ç†è§£ä¸Šä¸‹æ–‡ã€‚
                - è¯·æ³¨æ„ï¼Œä½ ä¼šå°è¯•è”æƒ³å›å¿†å’Œç›®å‰äº’åŠ¨æœ‰å…³çš„è®°å¿†ï¼Œæ‰€ä»¥æœ‰**é•¿æœŸè®°å¿†**å¯ä»¥å‚è€ƒï¼Œä½†è¿™äº›è®°å¿†ä¸­æœ‰æ—¶å­˜åœ¨è”æƒ³åˆ°çš„æ— å…³å†…å®¹ã€‚
                - è‹¥å¯¹è¯å†å²å’Œé•¿æœŸè®°å¿†ä¿¡æ¯æœ‰å†²çªï¼Œä¼˜å…ˆä½¿ç”¨å¯¹è¯å†å²çš„ä¿¡æ¯ã€‚
                - ä½ æ”¶åˆ°çš„è§†è§‰å›¾ç‰‡è¾“å…¥æ¥è‡ªä½ çš„æ‘„åƒå¤´ï¼Œæ¯æ¬¡å¯¹è¯æ—¶éƒ½ä¼šè·å¾—ä¸€å¼ å½“å‰æ‘„åƒå¤´çœ‹åˆ°çš„ç…§ç›¸ã€‚
                - ä½ åº”è¯¥è‡ªè¡Œåˆ¤æ–­å†å²å’Œå›¾ç‰‡ä¿¡æ¯æ˜¯å¦ä¸å½“å‰å¯¹è¯ç›¸å…³ï¼Œå¹¶è‡ªç„¶åœ°å°†*çœŸæ­£ç›¸å…³*çš„ä¿¡æ¯èå…¥åˆ°ä½ çš„è¯­è¨€å›å¤ä¸­ã€‚
                
                - é€‰æ‹©åˆé€‚çš„è¡¨æƒ…åç§°ã€åŠ¨ä½œåç§°åŠ å…¥åˆ° JSON ç»“æ„ä¸­ã€‚
                    å¯ç”¨è¡¨æƒ…ï¼š["é»‘è„¸", "ç™½çœ¼", "æ‹¿æ——å­", "çœ¼æ³ª"]
                    å¯ç”¨åŠ¨ä½œï¼š["å¥½å¥‡", "çŒç¡", "å®³æ€•", "ä¸¾ç™½æ——", "æ‘‡å¤´", "æŠ±æ•å¤´"]

                è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ¼å¼è¿”å›ï¼š
                1. é¦–å…ˆç”¨ ```meta åŒ…è£¹JSONå…ƒæ•°æ®ï¼ˆå¿…é¡»ç‹¬ç«‹æˆchunkï¼‰
                2. éšåæ˜¯è‡ªç„¶è¯­è¨€å›å¤
                ```meta
                {{ "reasoning":â€œæ€è€ƒè¿‡ç¨‹ï¼ˆæ‹ŸäººåŒ–æ€è€ƒï¼‰â€, "expression":"è¡¨æƒ…åç§°", "motion":"åŠ¨ä½œåç§°"}}
                [ä½ çš„è‡ªç„¶è¯­è¨€å›å¤]
                
                === åŠ¨æ€ä¿¡æ¯ ===
                **å¯¹è¯å†å²**:
                ```json
                {history_json}
                ```
                **é•¿æœŸè®°å¿†**ï¼š
                {long_term_memories}

                **å½“å‰ç”¨æˆ·è¾“å…¥**:
                ```text
                {user_input}
                ```
                
                **ç³»ç»Ÿæ—¶é—´**
                {timestamp}         
                """

        # 3. å‡†å¤‡å†…å®¹ (parts)
        parts = [{"text": system_instruction}]
        if image_base64:
            try:
                image_data = base64.b64decode(image_base64)
                image = Image.open(io.BytesIO(image_data))
                parts.append({
                    "inline_data": {
                        "mime_type": "image/jpeg",
                        "data": image_data
                    }
                })
            except Exception as e:
                print(f"Base64 å›¾åƒæ•°æ®è§£ç å¤±è´¥: {str(e)}")

        # 4. è°ƒç”¨ LLM
        start_time_gemini = time.time()
        #gemini_response = client.models.generate_content_stream(model="gemini-2.0-pro-exp-02-05",
        #                                                        contents=[system_instruction, image])


        # 5. è§£æ JSON å“åº”
        model = genai.GenerativeModel('gemini-2.0-pro-exp-02-05')


        async for chunk in await model.generate_content_async(contents=parts,stream=True):
            raw_text = chunk.text
            print(f"ğŸ”´ åŸå§‹å“åº”å—: {repr(raw_text)}")

            # å¤„ç†å…ƒæ•°æ®å—
            processed_text = meta_parser.feed(raw_text)

            # å¤„ç†æ–‡æœ¬æµ
            text_buffer += processed_text

            # å®æ—¶åˆ†å‰²
            while True:
                segment, remaining = split_text_stream(text_buffer)
                if not segment:
                    break

                print(f"ğŸŸ¡ ç”Ÿæˆæ®µè½: {segment}")
                yield {
                    "type": "segment",
                    "segment": segment,
                    "expression": meta_parser.metadata["expression"],
                    "motion": meta_parser.metadata["motion"]
                }
                text_buffer = remaining

            # å¤„ç†å‰©ä½™å†…å®¹
        if text_buffer.strip():
            yield {
                "type": "segment",
                "segment": text_buffer.strip(),
                "expression": meta_parser.metadata["expression"],
                "motion": meta_parser.metadata["motion"]
            }
    except Exception as e:
        print(f"Stream error: {e}")


