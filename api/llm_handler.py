# llm_handler.py
import google.generativeai as genai
#from google import genai
from dotenv import load_dotenv
import os
import json
import weaviate
from mem0 import Memory  # å¯¼å…¥ Mem0
import logging
import base64 # å¯¼å…¥ base64 åº“ï¼Œè™½ç„¶è¿™é‡Œå¯èƒ½ä¸æ˜¯å¿…é¡»çš„ï¼Œä½†åœ¨å¤„ç†å›¾åƒæ•°æ®æ—¶ï¼Œå¯¼å…¥æ€»æ˜¯æœ‰å¤‡æ— æ‚£
import time
from utils.utils import load_api_key
from datetime import datetime
import asyncio
from PIL import Image
import io

load_dotenv()

# åˆå§‹åŒ– Weaviate å®¢æˆ·ç«¯ (ä¿æŒä¸å˜)
weaviate_client = weaviate.connect_to_local()

# åˆå§‹åŒ– Gemini (ä¿æŒä¸å˜)
GEMINI_API_KEY = load_api_key("GEMINI_API_KEY")
#client = genai.Client(api_key=GEMINI_API_KEY)
os.environ["GEMINI_API_KEY"] = GEMINI_API_KEY

# åˆå§‹åŒ– Mem0 (ä¿æŒä¸å˜)
config = {
    "vector_store": {
        "provider": "qdrant",
        "config": {
            "collection_name": "test1",
            "host": "localhost",
            "port": 6333,
            "embedding_model_dims": 1024,
        }
    },
    "llm": {
        "provider": "gemini",
        "config": {
            "model": "gemini-2.0-flash",
            "temperature": 0.2,
            "max_tokens": 1500,
        }
    },
    "embedder": {
        "provider": "ollama",
        "config": {
            "model": "mxbai-embed-large:latest",
        }
    }
}
#mem0 = Memory.from_config(config)

# è¯»å–è§’è‰²è®¾å®šæ–‡ä»¶ (ä¿æŒä¸å˜)
with open("../Prompt/Character/Lily.txt", "r", encoding="utf-8") as file:
    character_profile = file.read()

# æŸ¥è¯¢é•¿æœŸè®°å¿† (ä¿æŒä¸å˜)
def query_long_term_memory_input(user_input):
    related_memory = []
    for collection_name in ["Events", "Relationships", "Knowledge", "Goals", "Preferences", "Profile"]:
        collection = weaviate_client.collections.get(collection_name)
        existing_mem = collection.query.hybrid(
            query=f"User: {user_input}",
            limit=2
        )
        related_memory.append(existing_mem)
    return related_memory


# TODO: check if adding user_id in prompt can let AI distinguish user identity in future memory
def llm_response_generation(user_input, user_id, user_chat_sessions, manual_history, image_base64=None): # æ·»åŠ  manual_history å‚æ•°
    """
    ä¸» LLM å›å¤ç”Ÿæˆå‡½æ•° (ä½¿ç”¨æ‰‹åŠ¨ç»´æŠ¤çš„å¯¹è¯å†å²).

    Args:
        user_input (str): ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
        user_id (str): ç”¨æˆ· ID
        user_chat_sessions (dict): ä»ç„¶ä¿ç•™ï¼Œä½†å¯èƒ½ä¸å†ç›´æ¥ä½¿ç”¨ ChatSession å¯¹è±¡ (å¯ä»¥ç”¨äºå…¶ä»–ç”¨æˆ·ç›¸å…³æ•°æ®å­˜å‚¨)
        manual_history (list): æ‰‹åŠ¨ç»´æŠ¤çš„å¯¹è¯å†å²åˆ—è¡¨  <- æ–°å¢ manual_history å‚æ•°
        image_base64 (str, optional): ç”¨æˆ·è¾“å…¥çš„å›¾åƒ Base64 å­—ç¬¦ä¸²ï¼Œé»˜è®¤ä¸º None

    Returns:
        dict: åŒ…å«å›å¤ä¿¡æ¯çš„å­—å…¸ (response_text, expression, motion ç­‰)
    """
    # 1.  ä¸å†éœ€è¦è·å–æˆ–åˆ›å»º ChatSession å¯¹è±¡

    # 2. è°ƒç”¨ get_gemini_response_with_history è·å– Gemini å›å¤ (ä¿®æ”¹éƒ¨åˆ†)
    gemini_response_dict = get_gemini_response_with_history(user_input, user_id, manual_history, image_base64=image_base64) # ä¼ é€’ manual_history

    # 3. è¿”å›ç»“æœ (ä¿æŒä¸å˜)
    return gemini_response_dict


class MetadataParser:
    def __init__(self):
        self.meta_buffer = ""
        self.in_meta_block = False
        self.metadata = {"expression": "normal", "motion": "idle"}

    def feed(self, chunk_text):
        segments = []

        if not self.in_meta_block:
            # æ£€æµ‹å…ƒæ•°æ®å—èµ·å§‹æ ‡è®°
            if "```meta" in chunk_text:
                self.in_meta_block = True
                chunk_text = chunk_text.split("```meta")[-1]

        if self.in_meta_block:
            # æ£€æµ‹å…ƒæ•°æ®å—ç»“æŸæ ‡è®°
            if "```" in chunk_text:
                meta_part, remaining = chunk_text.split("```", 1)
                self.meta_buffer += meta_part

                try:
                    self.metadata.update(json.loads(self.meta_buffer))
                    print(f"ğŸŸ¢ æˆåŠŸè§£æå…ƒæ•°æ®: {self.metadata}")
                except Exception as e:
                    print(f"ğŸ”´ å…ƒæ•°æ®è§£æå¤±è´¥: {e}")

                # é‡ç½®çŠ¶æ€
                self.in_meta_block = False
                self.meta_buffer = ""

                # è¿”å›å…ƒæ•°æ®åçš„å‰©ä½™æ–‡æœ¬
                return remaining.strip()
            else:
                self.meta_buffer += chunk_text
                return ""
        else:
            return chunk_text


def split_text_stream(buffer):
    split_chars = ['ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ', '...']
    positions = []

    # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„åˆ†å‰²ç‚¹
    for char in split_chars:
        pos = buffer.find(char)
        while pos != -1:
            positions.append(pos + len(char) - 1)
            pos = buffer.find(char, pos + 1)

    if not positions:
        return None, buffer

    # é€‰æ‹©æœ€åˆé€‚çš„åˆ†å‰²ç‚¹ï¼ˆä¼˜å…ˆååŠéƒ¨åˆ†çš„æ ‡ç‚¹ï¼‰
    optimal_pos = max(
        [p for p in positions if p < len(buffer) * 0.8],
        default=max(positions, default=-1)
    )

    if optimal_pos == -1:
        return None, buffer

    return buffer[:optimal_pos + 1].strip(), buffer[optimal_pos + 1:].lstrip()

async def get_gemini_response_with_history(user_input, user_id, manual_history, image_base64=None): # ä¿®æ”¹å‡½æ•°ç­¾åï¼Œæ·»åŠ  manual_history
    """
    ä½¿ç”¨ æ‰‹åŠ¨ç»´æŠ¤çš„å¯¹è¯å†å²ï¼Œè°ƒç”¨ Gemini ç”Ÿæˆå›å¤.

    Args:
        user_input (str): ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
        user_id (str): ç”¨æˆ· ID
        manual_history (list): æ‰‹åŠ¨ç»´æŠ¤çš„å¯¹è¯å†å²åˆ—è¡¨  <- æ–°å¢ manual_history å‚æ•°
        image_base64 (str, optional): ç”¨æˆ·è¾“å…¥çš„å›¾åƒ Base64 å­—ç¬¦ä¸²ï¼Œé»˜è®¤ä¸º None
    """
    try:
        # åˆå§‹åŒ–çŠ¶æ€
        text_buffer = ""
        meta_parser = MetadataParser()
        # 1. æ£€ç´¢è®°å¿† (ä¸­æœŸå’Œé•¿æœŸ) -  æ¯æ¬¡éƒ½é‡æ–°æ£€ç´¢ (ä¿æŒä¸å˜)
        # mid_term_memories = mem0.search(query=user_input, user_id="default_user", limit=3)
        # memories_str = "\n".join(f"- {entry['memory']}" for entry in mid_term_memories)
        long_term_memories = query_long_term_memory_input(user_input)

        # 1. æ„å»ºå¯¹è¯å†å²çš„ JSON ç»“æ„
        history_json = json.dumps(manual_history, ensure_ascii=False, indent=2)  # ä¿æŒ JSON æ ¼å¼ï¼Œé¿å… ASCII è½¬ä¹‰
        timestamp = datetime.now().isoformat()
        # 2. ç”Ÿæˆ Prompt
        system_instruction = f"""
                === ä½ çš„è§’è‰²æ¡£æ¡ˆ ===
                {character_profile}

                === å½“å‰ç”¨æˆ· ===
                {user_id}ï¼Œèº«ä»½æ˜¯ä½ æ›¾ç»çš„åŒå­¦ã€‚
                
                === LLM ä»»åŠ¡è¦æ±‚ ===
                ä½ å°†å®Œå…¨ä»£å…¥ä½ çš„è§’è‰²æ¡£æ¡ˆï¼Œæˆä¸ºä½ æ‰®æ¼”çš„äººï¼Œåœ¨æ­¤åŸºç¡€ä¸Šï¼š
                - è¯·åŸºäºç”¨æˆ·è¾“å…¥å’Œ *å¯¹è¯å†å²* ç”Ÿæˆä½ çš„å›å¤ã€‚
                - **å¯¹è¯å†å²**ä¸­ä½ åº”è¯¥æ›´å…³æ³¨æœ€è¿‘çš„æ¶ˆæ¯ï¼Œä½†ä»ç„¶å¯ä»¥ç»“åˆæ•´ä¸ªå¯¹è¯å†å²æ¥ç†è§£ä¸Šä¸‹æ–‡ã€‚
                - è¯·æ³¨æ„ï¼Œä½ ä¼šå°è¯•è”æƒ³å›å¿†å’Œç›®å‰äº’åŠ¨æœ‰å…³çš„è®°å¿†ï¼Œæ‰€ä»¥æœ‰**é•¿æœŸè®°å¿†**å¯ä»¥å‚è€ƒï¼Œä½†è¿™äº›è®°å¿†ä¸­æœ‰æ—¶å­˜åœ¨è”æƒ³åˆ°çš„æ— å…³å†…å®¹ã€‚
                - è‹¥å¯¹è¯å†å²å’Œé•¿æœŸè®°å¿†ä¿¡æ¯æœ‰å†²çªï¼Œä¼˜å…ˆä½¿ç”¨å¯¹è¯å†å²çš„ä¿¡æ¯ã€‚
                - ä½ æ”¶åˆ°çš„è§†è§‰å›¾ç‰‡è¾“å…¥æ¥è‡ªä½ çš„æ‘„åƒå¤´ï¼Œæ¯æ¬¡å¯¹è¯æ—¶éƒ½ä¼šè·å¾—ä¸€å¼ å½“å‰æ‘„åƒå¤´çœ‹åˆ°çš„ç…§ç›¸ã€‚
                - ä½ åº”è¯¥è‡ªè¡Œåˆ¤æ–­å†å²å’Œå›¾ç‰‡ä¿¡æ¯æ˜¯å¦ä¸å½“å‰å¯¹è¯ç›¸å…³ï¼Œå¹¶è‡ªç„¶åœ°å°†*çœŸæ­£ç›¸å…³*çš„ä¿¡æ¯èå…¥åˆ°ä½ çš„è¯­è¨€å›å¤ä¸­ã€‚
                
                - é€‰æ‹©åˆé€‚çš„è¡¨æƒ…åç§°ã€åŠ¨ä½œåç§°åŠ å…¥åˆ° JSON ç»“æ„ä¸­ã€‚
                    å¯ç”¨è¡¨æƒ…ï¼š["é»‘è„¸", "ç™½çœ¼", "æ‹¿æ——å­", "çœ¼æ³ª"]
                    å¯ç”¨åŠ¨ä½œï¼š["å¥½å¥‡", "çŒç¡", "å®³æ€•", "ä¸¾ç™½æ——", "æ‘‡å¤´", "æŠ±æ•å¤´"]

                è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ¼å¼è¿”å›å“åº”ï¼š
                1. é¦–å…ˆç”¨ ```meta åŒ…è£¹JSONå…ƒæ•°æ®ï¼ˆå¿…é¡»å•ç‹¬æˆå—ï¼‰
                2. éšåæ˜¯è‡ªç„¶è¯­è¨€å›å¤ï¼ˆæŒ‰æ ‡ç‚¹åˆ†å—è¾“å‡ºï¼‰
                ```meta
                {{ "reasoning":"æ€è€ƒè¿‡ç¨‹ï¼ˆæ‹ŸäººåŒ–æ€è€ƒï¼‰","expression":"è¡¨æƒ…åç§°", "motion":"åŠ¨ä½œåç§°"}}
                [ä½ çš„è‡ªç„¶è¯­è¨€å›å¤]
                
                === åŠ¨æ€ä¿¡æ¯ ===
                **å¯¹è¯å†å²**:
                ```json
                {history_json}
                ```
                **é•¿æœŸè®°å¿†**ï¼š
                {long_term_memories}

                **å½“å‰ç”¨æˆ·è¾“å…¥**:
                ```text
                {user_input}
                ```
                
                **ç³»ç»Ÿæ—¶é—´**
                {timestamp}         
                """

        # 3. å‡†å¤‡å†…å®¹ (parts)
        parts = [{"text": system_instruction}]
        if image_base64:
            try:
                image_data = base64.b64decode(image_base64)
                image = Image.open(io.BytesIO(image_data))
                parts.append({
                    "inline_data": {
                        "mime_type": "image/jpeg",
                        "data": image_data
                    }
                })
            except Exception as e:
                print(f"Base64 å›¾åƒæ•°æ®è§£ç å¤±è´¥: {str(e)}")

        # 4. è°ƒç”¨ LLM
        start_time_gemini = time.time()
        #gemini_response = client.models.generate_content_stream(model="gemini-2.0-pro-exp-02-05",
        #                                                        contents=[system_instruction, image])


        # 5. è§£æ JSON å“åº”
        model = genai.GenerativeModel('gemini-2.0-pro-exp-02-05')


        # å¼‚æ­¥å¤„ç†æµå¼å“åº”
        json_buffer = ""
        response_text_buffer = ""
        header_parsed = False
        result = {
            "expression": "normal",
            "motion": "idle",
            "reasoning": "",
            "response_text": ""
        }

        async for chunk in await model.generate_content_async(contents=parts,stream=True):
            raw_text = chunk.text
            print(f"ğŸ”´ åŸå§‹å“åº”å—: {repr(raw_text)}")

            # å¤„ç†å…ƒæ•°æ®å—
            processed_text = meta_parser.feed(raw_text)

            # å¤„ç†æ–‡æœ¬æµ
            text_buffer += processed_text

            # å®æ—¶åˆ†å‰²
            while True:
                segment, remaining = split_text_stream(text_buffer)
                if not segment:
                    break

                print(f"ğŸŸ¡ ç”Ÿæˆæ®µè½: {segment}")
                yield {
                    "type": "segment",
                    "segment": segment,
                    "expression": meta_parser.metadata["expression"],
                    "motion": meta_parser.metadata["motion"]
                }
                text_buffer = remaining

            # å¤„ç†å‰©ä½™å†…å®¹
        if text_buffer.strip():
            yield {
                "type": "segment",
                "segment": text_buffer.strip(),
                "expression": meta_parser.metadata["expression"],
                "motion": meta_parser.metadata["motion"]
            }
    except Exception as e:
        print(f"Stream error: {e}")


